{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 21:48:34.567295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # stemmer it will remove a few words from the word and try to get to the base word\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "caring  |  care\n",
      "capable  |  capabl\n",
      "ate  |  ate\n"
     ]
    }
   ],
   "source": [
    "#example \n",
    "words = [\"running\", \"caring\", \"capable\", \"ate\"]\n",
    "for word in words:\n",
    "    print(word,\" | \", stemmer.stem(word))\n",
    "# it's not that affective since it doesn't work according to the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shaun  |  Shaun\n",
      "Murphy  |  Murphy\n",
      ",  |  ,\n",
      "a  |  a\n",
      "young  |  young\n",
      "surgeon  |  surgeon\n",
      "with  |  with\n",
      "autism  |  autism\n",
      "and  |  and\n",
      "savant  |  savant\n",
      "syndrome  |  syndrome\n",
      ",  |  ,\n",
      "relocates  |  relocate\n",
      "from  |  from\n",
      "a  |  a\n",
      "quiet  |  quiet\n",
      "country  |  country\n",
      "life  |  life\n",
      "to  |  to\n",
      "join  |  join\n",
      "the  |  the\n",
      "surgical  |  surgical\n",
      "unit  |  unit\n",
      "at  |  at\n",
      "the  |  the\n",
      "prestigious  |  prestigious\n",
      "San  |  San\n",
      "Jose  |  Jose\n",
      "St.  |  St.\n",
      "Bonaventure  |  Bonaventure\n",
      "Hospital  |  Hospital\n",
      "--  |  --\n",
      "a  |  a\n",
      "move  |  move\n",
      "strongly  |  strongly\n",
      "supported  |  support\n",
      "by  |  by\n",
      "his  |  his\n",
      "mentor  |  mentor\n",
      ",  |  ,\n",
      "Dr.  |  Dr.\n",
      "Aaron  |  Aaron\n",
      "Glassman  |  Glassman\n",
      ".  |  .\n",
      "Having  |  having\n",
      "survived  |  survive\n",
      "a  |  a\n",
      "troubled  |  troubled\n",
      "childhood  |  childhood\n",
      ",  |  ,\n",
      "Shaun  |  Shaun\n",
      "is  |  be\n",
      "alone  |  alone\n",
      "in  |  in\n",
      "the  |  the\n",
      "world  |  world\n",
      "and  |  and\n",
      "unable  |  unable\n",
      "to  |  to\n",
      "personally  |  personally\n",
      "connect  |  connect\n",
      "with  |  with\n",
      "those  |  those\n",
      "around  |  around\n",
      "him  |  he\n",
      ",  |  ,\n",
      "but  |  but\n",
      "he  |  he\n",
      "finds  |  find\n",
      "his  |  his\n",
      "niche  |  niche\n",
      "using  |  use\n",
      "his  |  his\n",
      "extraordinary  |  extraordinary\n",
      "medical  |  medical\n",
      "skill  |  skill\n",
      "and  |  and\n",
      "intuition  |  intuition\n",
      "to  |  to\n",
      "save  |  save\n",
      "lives  |  life\n",
      "and  |  and\n",
      "challenge  |  challenge\n",
      "the  |  the\n",
      "skepticism  |  skepticism\n",
      "of  |  of\n",
      "his  |  his\n",
      "colleagues  |  colleague\n",
      ".  |  .\n",
      "\n",
      "\n",
      "  |  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy we can get the base word according to the language we are using, here it's english\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp('''Shaun Murphy, a young surgeon with autism and savant syndrome, relocates from a quiet country life to join the surgical unit at the prestigious San Jose St. Bonaventure Hospital -- a move strongly supported by his mentor, Dr. Aaron Glassman. Having survived a troubled childhood, Shaun is alone in the world and unable to personally connect with those around him, but he finds his niche using his extraordinary medical skill and intuition to save lives and challenge the skepticism of his colleagues.\n",
    "\n",
    "''')\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruh  |  Brother\n",
      ",  |  ,\n",
      "I  |  I\n",
      "do  |  do\n",
      "n't  |  not\n",
      "think  |  think\n",
      "that  |  that\n",
      "you  |  you\n",
      "should  |  should\n",
      "eat  |  eat\n",
      "banana  |  banana\n",
      "when  |  when\n",
      "you  |  you\n",
      "are  |  be\n",
      "sick  |  sick\n"
     ]
    }
   ],
   "source": [
    "# attribute_ruler to get customised lemma for slangs that people use like 'bruh' since it won't be in the language module\n",
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{\"TEXT\" : \"Bro\"}],[{\"TEXT\":\"bruh\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"bruh, I don't think that you should eat banana when you are sick\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,\" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
